# lambda/analyze/handler.py
import os
import json
import re
import uuid
import boto3
import urllib.request
import urllib.error
from datetime import datetime, timedelta, timezone
from collections import Counter, defaultdict
from urllib.parse import urlparse
from decimal import Decimal, ROUND_HALF_UP
from boto3.dynamodb.conditions import Key, Attr
from botocore.exceptions import ClientError

DDB = boto3.resource("dynamodb")
CW = boto3.client("cloudwatch")
S3 = boto3.client("s3")

ANALYTICS_BUCKET = os.getenv("ANALYTICS_BUCKET", "")
ANALYTICS_PREFIX = os.getenv("ANALYTICS_PREFIX", "analytics").strip("/")  # "analytics"
EXPORT_ENABLED = os.getenv("EXPORT_ENABLED", "false").lower() == "true"
EXPORT_CHECKPOINT_KEY = os.getenv("EXPORT_CHECKPOINT_KEY", f"{ANALYTICS_PREFIX}/state/last_export_ts.json")

# Bedrock Runtime (ì„œìš¸: ap-northeast-2ì—ì„œ ì§€ì›) - ëª¨ë¸IDëŠ” envë¡œ ì£¼ì…
BEDROCK_RUNTIME = boto3.client("bedrock-runtime")

URLS_TABLE = os.environ["URLS_TABLE"]
CLICKS_TABLE = os.environ["CLICKS_TABLE"]
INSIGHTS_TABLE = os.environ["INSIGHTS_TABLE"]
AI_TABLE = os.environ["AI_TABLE"]

MODEL_TREND = os.getenv("BEDROCK_MODEL_TREND", "amazon.nova-micro-v1:0")
MODEL_INSIGHT = os.getenv("BEDROCK_MODEL_INSIGHT", "amazon.nova-lite-v1:0")

TOP_N_REFERER = int(os.getenv("TOP_N_REFERER", "5"))
MAX_URLS_PER_RUN = int(os.getenv("MAX_URLS_PER_RUN", "200"))  # í•œ ë²ˆì— ë„ˆë¬´ ë§ì´ ëŒë¦¬ì§€ ì•Šê²Œ
ENABLE_AI_DEFAULT = os.getenv("ENABLE_AI_DEFAULT", "false").lower() == "true"

AI_TOP_URL_N = int(os.getenv("AI_TOP_URL_N", "20"))
AI_TOP_TIMEBIN_N = int(os.getenv("AI_TOP_TIMEBIN_N", "10"))
AI_SOURCE_PERIOD_DEFAULT = os.getenv("AI_SOURCE_PERIOD_DEFAULT", "P#24H")
MAX_CLICKS_PER_SID = int(os.getenv("MAX_CLICKS_PER_SID", "1000"))

KST = timezone(timedelta(hours=9))


SLACK_WEBHOOK_URL = os.getenv("SLACK_WEBHOOK_URL", "")
ALERT_ONLY_PERIOD = os.getenv("ALERT_ONLY_PERIOD", "P#1H")
ALERT_STATE_KEY = os.getenv(
    "ALERT_STATE_KEY",
    "analytics/state/alert_last_suspicious_by_sid_p1h.json"
)
MAX_ALERTS_PER_RUN = int(os.getenv("MAX_ALERTS_PER_RUN", "5"))

# suspicious rule thresholds
SUSP_WINDOW_SEC = int(os.getenv("SUSP_WINDOW_SEC", "60"))
SUSP_REPEAT_THRESHOLD = int(os.getenv("SUSP_REPEAT_THRESHOLD", "10"))

BOT_UA_PAT = re.compile(r"(bot|spider|crawler|headless|python-requests|curl|wget)", re.I)

COMMON_2LEVEL_SUFFIX = {
    "co.kr", "or.kr", "go.kr", "ac.kr",
    "co.jp", "ne.jp", "or.jp",
    "co.uk", "org.uk", "ac.uk",
    "com.au", "net.au", "org.au",
}

def lambda_handler(event, context):
    event = event or {}

    rc = event.get("requestContext") or {}
    http = rc.get("http") or {}
    method = http.get("method")  # HTTP APIë©´ ì¡´ì¬
    stage = rc.get("stage") or ""  # ì˜ˆ: "prod"
    raw_path = event.get("rawPath") or ""

    # -------------------------
    # 1) HTTP API ë¼ìš°íŒ… ì²˜ë¦¬
    # -------------------------
    if method:
        # CORS preflight
        if method == "OPTIONS":
            return _resp(200, {})

        # âœ… stage prefix(/prod) ì œê±°: "/prod/ai/latest" -> "/ai/latest"
        if stage and raw_path.startswith(f"/{stage}/"):
            path = raw_path[len(stage) + 1:]  # "/prod" ê¸¸ì´ë§Œí¼ ì œê±°í•˜ê³  "/" ìœ ì§€
        elif stage and raw_path == f"/{stage}":
            path = "/"
        else:
            path = raw_path

        # (ë””ë²„ê·¸ìš©) ì ê¹ ì¼œë‘ë©´ ì›ì¸ ë°”ë¡œ ë³´ì„
        print(json.dumps({
            "type": "HTTP_API_IN",
            "method": method,
            "stage": stage,
            "rawPath": raw_path,
            "normalizedPath": path,
            "routeKey": rc.get("routeKey"),
            "query": event.get("queryStringParameters"),
        }, ensure_ascii=False))

        if path == "/ai/latest" and method == "GET":
            period_key = _get_query(event, "periodKey", "P#30MIN").upper()
            allowed = {"P#1MIN", "P#5MIN", "P#30MIN", "P#1H", "P#24H", "P#7D"}
            if period_key not in allowed:
                return _resp(400, {"message": "INVALID_periodKey", "allowed": sorted(list(allowed))})
            return _resp(200, get_latest_ai(period_key))

        return _resp(404, {"message": "NOT_FOUND"})

    # ---------------------------------
    # 2) EventBridge / ìˆ˜ë™ invoke ì²˜ë¦¬
    # ---------------------------------
    job = event.get("job", "aggregate_only")

    if job == "ai_only":
        ai_period_key = event.get("aiPeriodKey", "P#30MIN")
        source_period_key = event.get("sourcePeriodKey", AI_SOURCE_PERIOD_DEFAULT)
        result = run_ai_job(ai_period_key, source_period_key)
        print(json.dumps({"type": "AI_ONLY_RESULT", "result": result}, ensure_ascii=False))
        return _resp(200, result)

    period_key = event.get("periodKey", "P#1H")
    result = run_aggregation(period_key)
    print(json.dumps({"type": "ANALYZE_RESULT", "result": result}, ensure_ascii=False))
    return _resp(200, result)







def extract_domain(original_url: str) -> str:
    if not original_url:
        return ""
    u = original_url.strip()
    if "://" not in u:
        u = "https://" + u

    p = urlparse(u)
    host = (p.netloc or "").strip().lower()

    # netlocì´ ë¹„ëŠ” ì¼€ì´ìŠ¤ ë³´ì • (rare)
    if not host and p.path:
        host = p.path.split("/")[0].lower()

    # userinfo ì œê±°
    if "@" in host:
        host = host.split("@", 1)[1]

    # port ì œê±°
    if ":" in host:
        host = host.split(":", 1)[0]

    # www ì œê±°
    if host.startswith("www."):
        host = host[4:]

    if "." not in host:
        return ""
    return host

def to_root_domain(host: str) -> str:
    """
    ë¹„ìš© ìµœì†Œë¥¼ ìœ„í•´ tldextract ê°™ì€ ì™¸ë¶€ ë¼ì´ë¸ŒëŸ¬ë¦¬ ì—†ì´ "ëŒ€ë¶€ë¶„ ë§ëŠ”" ë£¨íŠ¸ ë„ë©”ì¸ë§Œ ì²˜ë¦¬.
    (notion.site, youtube.com ê°™ì´ UIì—ì„œ ì›í•˜ëŠ” í˜•íƒœ)
    """
    if not host:
        return ""
    parts = host.split(".")
    if len(parts) < 2:
        return ""

    last2 = ".".join(parts[-2:])
    last3 = ".".join(parts[-3:])

    # co.kr / co.uk ê°™ì€ 2ë‹¨ suffixë©´ ë§ˆì§€ë§‰ 3ê°œë¥¼ ë£¨íŠ¸ë¡œ
    if last2 in COMMON_2LEVEL_SUFFIX and len(parts) >= 3:
        return last3

    return last2


def normalize_url(u: str, max_len: int = 140) -> str:
    """AI ì…ë ¥ í† í° í­ë°œ ë°©ì§€: query/fragment ì œê±° + ë„ˆë¬´ ê¸¸ë©´ ìë¥´ê¸°"""
    if not u:
        return ""
    s = u.strip()
    if "://" not in s:
        s = "https://" + s
    p = urlparse(s)
    base = f"{p.scheme}://{p.netloc}{p.path}"
    return base[:max_len] if len(base) > max_len else base


def to_5min_slot(ts_iso: str) -> str:
    """KST(Asia/Seoul) ê¸°ì¤€ 5ë¶„ ìŠ¬ë¡¯ -> 'HH:MM'"""
    dt_utc = datetime.strptime(ts_iso, "%Y-%m-%dT%H:%M:%SZ").replace(tzinfo=timezone.utc)
    dt_kst = dt_utc.astimezone(KST)

    m = (dt_kst.minute // 5) * 5
    return f"{dt_kst.hour:02d}:{m:02d}"


def now_utc():
    return datetime.now(timezone.utc)


def iso(dt: datetime) -> str:
    # DynamoDB range_key(timestamp)ì™€ ì •ë ¬/ë¹„êµê°€ ì•ˆì „í•˜ë„ë¡ ISO 8601 Z í˜•íƒœ ìœ ì§€ ê¶Œì¥
    return dt.strftime("%Y-%m-%dT%H:%M:%SZ")


def parse_period_key(period_key: str):
    """
    periodKey ì˜ˆ: P#1MIN / P#1H / P#24H / P#7D
    """
    period_key = period_key.upper()
    if period_key == "P#1MIN":
        return timedelta(minutes=1)
    if period_key == "P#5MIN":          # âœ… ì¶”ê°€
        return timedelta(minutes=5)
    if period_key == "P#30MIN":
        return timedelta(minutes=30)
    if period_key == "P#1H":
        return timedelta(hours=1)
    if period_key == "P#24H":
        return timedelta(hours=24)
    if period_key == "P#7D":
        return timedelta(days=7)
    raise ValueError(f"Unsupported periodKey: {period_key}")


def chunked(iterable, size):
    buf = []
    for x in iterable:
        buf.append(x)
        if len(buf) >= size:
            yield buf
            buf = []
    if buf:
        yield buf


def bedrock_invoke_text(model_id: str, user_text: str, max_tokens: int = 300):
    """
    Nova (Inference Profile) í˜¸ì¶œ: messages í¬ë§· í•„ìš”
    """
    body = {
        "messages": [
            {
                "role": "user",
                "content": [{"text": user_text}]
            }
        ],
        "inferenceConfig": {
            "maxTokens": max_tokens,
            "temperature": 0.2,
            "topP": 0.9
        }
    }

    resp = BEDROCK_RUNTIME.invoke_model(
        modelId=model_id,
        body=json.dumps(body).encode("utf-8"),
        accept="application/json",
        contentType="application/json",
    )

    raw = resp["body"].read()
    data = json.loads(raw)

    # Nova messages ì‘ë‹µì—ì„œ í…ìŠ¤íŠ¸ ì¶”ì¶œ (ë°©ì–´ì ìœ¼ë¡œ)
    if isinstance(data, dict):
        # ë³´í†µ: output.message.content[0].text
        out = data.get("output") or {}
        msg = out.get("message") or {}
        content = msg.get("content")
        if isinstance(content, list) and content:
            c0 = content[0]
            if isinstance(c0, dict) and "text" in c0:
                return c0["text"]

        # í˜¹ì‹œ ë‹¤ë¥¸ í‚¤ë¡œ ì˜¤ëŠ” ê²½ìš° ëŒ€ë¹„
        if "results" in data and data["results"]:
            r0 = data["results"][0]
            if isinstance(r0, dict):
                return r0.get("outputText") or r0.get("text") or json.dumps(data, ensure_ascii=False)

        for k in ("outputText", "completion", "generatedText", "text"):
            if k in data and isinstance(data[k], str):
                return data[k]

    return json.dumps(data, ensure_ascii=False)



def put_custom_metrics(namespace: str, metrics: dict, dims: list):
    # ì»¤ìŠ¤í…€ ë©”íŠ¸ë¦­ì€ ìˆ«ìë§Œ ê°€ëŠ¥ (ë¬¸ì¥/í…ìŠ¤íŠ¸ëŠ” ë¡œê·¸ë¡œ)
    metric_data = []
    for k, v in metrics.items():
        metric_data.append({
            "MetricName": k,
            "Dimensions": dims,
            "Timestamp": datetime.utcnow(),
            "Value": float(v),
            "Unit": "Count" if "Clicks" in k or "Count" in k else "None",
        })
    # PutMetricDataëŠ” í•œ ë²ˆì— ìµœëŒ€ 20ê°œ
    for batch in chunked(metric_data, 20):
        CW.put_metric_data(Namespace=namespace, MetricData=batch)


def compute_suspicious(click_items):
    """
    ë¹„ì •ìƒ í´ë¦­ ê°ì§€ ë£° (OR):
    1) bot UA íŒ¨í„´ í¬í•¨ (í´ë¦­ 1ê±´ ë‹¨ìœ„)
    2) ë™ì¼ ipHash + ë™ì¼ userAgentê°€ SUSP_WINDOW_SEC ë‚´ SUSP_REPEAT_THRESHOLD ì´ìƒ ë°˜ë³µ(burst)

    ë°˜í™˜: suspiciousClicks (ì¤‘ë³µ ì œê±°ëœ í´ë¦­ ê±´ìˆ˜)
    """

    # í´ë¦­ 1ê±´ì„ ìœ ë‹ˆí¬í•˜ê²Œ ì‹ë³„í•  í‚¤ (ì¤‘ë³µ ì¹´ìš´íŠ¸ ë°©ì§€)
    def click_key(it):
        return (
            it.get("timestamp", "") or "",
            it.get("ip", "") or "",
            it.get("userAgent", "") or "",
        )

    suspicious = set()

    # 1) bot UA: í´ë¦­ ë‹¨ìœ„ë¡œ ë°”ë¡œ suspicious ì²˜ë¦¬
    for it in click_items:
        ua = (it.get("userAgent") or "")
        if ua and BOT_UA_PAT.search(ua):
            suspicious.add(click_key(it))

    # 2) burst: (ip, ua) ê·¸ë£¹ë³„ ìŠ¬ë¼ì´ë”© ìœˆë„ìš°
    by_group = defaultdict(list)

    for it in click_items:
        ip_hash = it.get("ip", "") or ""
        ua = it.get("userAgent", "") or ""
        ts = it.get("timestamp", "") or ""

        if not (ip_hash and ua and ts):
            continue

        try:
            dt = datetime.strptime(ts, "%Y-%m-%dT%H:%M:%SZ").replace(tzinfo=timezone.utc)
        except Exception:
            continue

        by_group[(ip_hash, ua)].append((dt, it))

    for _, pairs in by_group.items():
        pairs.sort(key=lambda x: x[0])

        i = 0
        for j in range(len(pairs)):
            while pairs[j][0] - pairs[i][0] > timedelta(seconds=SUSP_WINDOW_SEC):
                i += 1

            window_size = j - i + 1
            if window_size >= SUSP_REPEAT_THRESHOLD:
                # ì´ ìœˆë„ìš° ì•ˆì˜ í´ë¦­ë“¤ì„ suspiciousë¡œ ì²˜ë¦¬ (ì¤‘ë³µì€ setì´ ì œê±°)
                for k in range(i, j + 1):
                    suspicious.add(click_key(pairs[k][1]))

                # ë¹„ìš©/ì—°ì‚° ìµœì†Œí™”ë¥¼ ìœ„í•´ ê·¸ë£¹ë‹¹ ì²« burst ë°œê²¬ ì‹œ ì¢…ë£Œ
                break
        # TODO(ì•Œë¦¼) - ì§€ê¸ˆì€ êµ¬í˜„í•˜ì§€ ì•ŠìŒ(ì£¼ì„ë§Œ)
        # [ì¶”ì²œ ì´ˆê¸° ì•ŒëŒ ê¸°ì¤€ - ê· í˜•]
        # - 1H(P#1H): totalClicks >= 100 AND suspiciousRate >= 0.35 (ì˜µì…˜: suspiciousClicks >= 30)
        # - 24H(P#24H): totalClicks >= 300 AND suspiciousRate >= 0.25 (ì˜µì…˜: suspiciousClicks >= 80)
        # - 1MIN(P#1MIN): ì•Œë¦¼ X (ë¡œê·¸/ëŒ€ì‹œë³´ë“œ í‘œì‹œë§Œ)
        #
        # êµ¬í˜„ í›„ë³´:
        # 1) CloudWatch Alarm: suspiciousRateë¥¼ ì»¤ìŠ¤í…€ ë©”íŠ¸ë¦­ìœ¼ë¡œ ì˜¬ë¦¬ê³ , Metric Mathë¡œ ì¡°ê±´ êµ¬ì„±
        # 2) insightsì— alertLevel í•„ë“œë§Œ ì €ì¥ í›„ í”„ë¡ íŠ¸ì—ì„œ ë°°ì§€ í‘œì‹œ

    return len(suspicious)
        

DEVICE_PATTERNS = {
    "mobile": re.compile(r"(iphone|ipod|android.*mobile|windows phone|blackberry|opera mini)", re.I),
    "tablet": re.compile(r"(ipad|android(?!.*mobile)|tablet)", re.I),
    "desktop": re.compile(r"(windows nt|macintosh|x11|linux)", re.I),
}

def classify_device(user_agent: str) -> str:
    ua = (user_agent or "").strip()
    if not ua:
        return "unknown"
    if BOT_UA_PAT.search(ua):
        return "bot"
    if DEVICE_PATTERNS["tablet"].search(ua):
        return "tablet"
    if DEVICE_PATTERNS["mobile"].search(ua):
        return "mobile"
    if DEVICE_PATTERNS["desktop"].search(ua):
        return "desktop"
    return "other"


def aggregate(click_items):
    
    """
    returns:
      totalClicks, clicksByHour(0-23), clicksByDay(YYYY-MM-DD), clicksByReferer(topN+other)
    """
    total = len(click_items)

    by_hour = Counter()
    by_day = Counter()
    by_ref = Counter()
    by_device = Counter()

    for it in click_items:
        ts = it.get("timestamp")
        ref = it.get("referer") or "direct"
        ua = it.get("userAgent") or ""
        try:
            dt_utc = datetime.strptime(ts, "%Y-%m-%dT%H:%M:%SZ").replace(tzinfo=timezone.utc)
            dt_kst = dt_utc.astimezone(KST)

            by_hour[f"{dt_kst.hour:02d}"] += 1        # âœ… KST + 2ìë¦¬
            by_day[dt_kst.strftime("%Y-%m-%d")] += 1  # âœ… KST ë‚ ì§œ
        except Exception:
            pass
        by_ref[ref] += 1

        device = classify_device(ua)    
        by_device[device] += 1

    # Top N referer + other
    top = by_ref.most_common(TOP_N_REFERER)
    top_keys = set([k for k, _ in top])

    compact_ref = {}
    other_sum = 0
    for k, v in by_ref.items():
        if k in top_keys:
            compact_ref[k] = v
        else:
            other_sum += v
    if other_sum > 0:
        compact_ref["other"] = other_sum

    return total, dict(by_hour), dict(by_day), compact_ref, dict(by_device)


def fetch_clicks_for_shortid(short_id: str, start_iso: str, end_iso: str, limit: int = 0):
    table = DDB.Table(CLICKS_TABLE)
    items = []
    last_key = None

    while True:
        kwargs = {
            "KeyConditionExpression": Key("shortId").eq(short_id) & Key("timestamp").between(start_iso, end_iso),
            "ProjectionExpression": "#ts, ip, userAgent, referer",
            "ExpressionAttributeNames": {"#ts": "timestamp"},
            "ScanIndexForward": False,  # ìµœì‹ ë¶€í„°
        }

        # limit ì²˜ë¦¬
        if limit and limit > 0:
            remaining = limit - len(items)
            if remaining <= 0:
                break
            kwargs["Limit"] = min(1000, remaining)

        if last_key:
            kwargs["ExclusiveStartKey"] = last_key

        resp = table.query(**kwargs)
        items.extend(resp.get("Items", []))
        last_key = resp.get("LastEvaluatedKey")

        if not last_key:
            break
        if limit and len(items) >= limit:
            break

    return items



def list_urls(limit: int):
    """
    urls í…Œì´ë¸”ì—ì„œ shortId ëª©ë¡ì„ ê°€ì ¸ì˜¨ë‹¤.
    ê·œëª¨ê°€ ì»¤ì§€ë©´ Scanì€ ë¹„ì‹¸ì§ -> ì§€ê¸ˆ ë‹¨ê³„(ê°œì¸ í”„ë¡œì íŠ¸/ì´ˆê¸°)ì—ì„œëŠ” ë‹¨ìˆœí™”.
    """
    table = DDB.Table(URLS_TABLE)
    items = []
    last_key = None

    while len(items) < limit:
        kwargs = {
            "ProjectionExpression": "shortId, title, clickCount, originalUrl",
            "Limit": min(100, limit - len(items))
        }
        if last_key:
            kwargs["ExclusiveStartKey"] = last_key

        resp = table.scan(**kwargs)
        items.extend(resp.get("Items", []))
        last_key = resp.get("LastEvaluatedKey")
        if not last_key:
            break

    return items


def upsert_insight(short_id: str, period_key: str, start_at: str, end_at: str,
                   total: int, by_hour: dict, by_day: dict, by_ref: dict, by_device: dict,
                   suspicious_clicks: int):
    table = DDB.Table(INSIGHTS_TABLE)
    if total == 0:
        suspicious_rate_dec = Decimal("0")
    else:
        suspicious_rate_dec = (Decimal(str(suspicious_clicks)) / Decimal(str(total))).quantize(
            Decimal("0.0001"),
            rounding=ROUND_HALF_UP
        )

    table.update_item(
        Key={"shortId": short_id, "periodKey": period_key},
        UpdateExpression="""
            SET startAt = :sa,
                endAt = :ea,
                totalClicks = :t,
                clicksByHour = :h,
                clicksByDay = :d,
                clicksByReferer = :r,
                clicksByDevice = :dv,
                generatedAt = :ga,
                suspiciousClicks = :sc,
                suspiciousRate = :sr
        """,
        ExpressionAttributeValues={
            ":sa": start_at,
            ":ea": end_at,
            ":t": int(total),
            ":h": by_hour,
            ":d": by_day,
            ":r": by_ref,
            ":dv": by_device,
            ":ga": iso(now_utc()),
            ":sc": int(suspicious_clicks),
            ":sr": suspicious_rate_dec,
        }
    )
    return float(suspicious_rate_dec)

def safe_json_obj(raw_text: str):
    if raw_text is None:
        return None
    if isinstance(raw_text, (dict, list)):
        return raw_text

    s = str(raw_text).strip()

    # ```json ... ``` ì•ˆìª½ë§Œ ë½‘ê¸° (ì•ë’¤ í…ìŠ¤íŠ¸ê°€ ìˆì–´ë„ OK)
    fence = re.search(r"```(?:json)?\s*(.*?)\s*```", s, flags=re.I | re.S)
    if fence:
        s = fence.group(1).strip()

    # ì²« JSON ê°ì²´/ë°°ì—´ë§Œ ë½‘ê¸°
    m = re.search(r"(\{[\s\S]*\}|\[[\s\S]*\])", s)
    if m:
        s = m.group(1).strip()

    try:
        return json.loads(s)
    except Exception:
        return {"raw": str(raw_text)}


def put_ai_result(period_key: str, ai_trend: dict | None, ai_insight: dict | None):
    """AI ê²°ê³¼ë¥¼ ai í…Œì´ë¸”ì— 'ëˆ„ì  ì €ì¥'"""
    if ai_trend is None and ai_insight is None:
        return

    table = DDB.Table(AI_TABLE)
    gen = iso(now_utc())

    item = {
        "periodKey": period_key,       # PK
        "aiGeneratedAt": gen,          # SK
    }
    if ai_trend is not None:
        item["aiTrend"] = ai_trend
    if ai_insight is not None:
        item["aiInsight"] = ai_insight

    table.put_item(Item=item)

def _scan_all_insights_for_period(period_key: str):
    """
    INSIGHTS_TABLEì—ì„œ periodKeyê°€ ê°™ì€ ëª¨ë“  shortId ì•„ì´í…œì„ scanìœ¼ë¡œ ê°€ì ¸ì˜¨ë‹¤.
    (ì´ˆê¸°/ê°œì¸í”„ë¡œì íŠ¸ ê·œëª¨ ì „ì œ. ì»¤ì§€ë©´ GSI ê¶Œì¥)
    """
    table = DDB.Table(INSIGHTS_TABLE)
    items = []
    last_key = None

    while True:
        kwargs = {
            "FilterExpression": Attr("periodKey").eq(period_key),
            "ProjectionExpression": "periodKey, clicksByHour",
        }
        if last_key:
            kwargs["ExclusiveStartKey"] = last_key

        resp = table.scan(**kwargs)
        items.extend(resp.get("Items", []))
        last_key = resp.get("LastEvaluatedKey")
        if not last_key:
            break

    return items


def build_global_hourly_timebins(period_key: str):
    """
    periodKey(P#30MIN ë“±) ê¸°ì¤€ìœ¼ë¡œ,
    ëª¨ë“  shortIdì˜ clicksByHourë¥¼ í•©ì‚°í•´ì„œ timeBins ë¦¬ìŠ¤íŠ¸ë¡œ ë°˜í™˜.
    output ì˜ˆ: [{"time":"06","clicks":12}, ... {"time":"21","clicks":95}]
    """
    rows = _scan_all_insights_for_period(period_key)

    # hour(0~23) í•©ì‚°
    summed = Counter()

    for it in rows:
        by_hour = it.get("clicksByHour") or {}
        if not isinstance(by_hour, dict):
            continue
        for h, c in by_hour.items():
            try:
                hh = int(h)  # "6" -> 6
                cc = int(c)
                if 0 <= hh <= 23:
                    summed[hh] += cc
            except Exception:
                continue

    # í”„ë¡ íŠ¸ ì°¨íŠ¸ìš©: 0~23 ì „ë¶€ ì±„ì›Œì„œ ë°˜í™˜(ë¹ˆ ì‹œê°„ëŒ€ 0)
    time_bins = []
    for hh in range(24):
        time_bins.append({
            "time": f"{hh:02d}",          # "06"
            "clicks": int(summed.get(hh, 0))
        })

    return time_bins


def get_latest_ai(period_key: str = "P#30MIN") -> dict:
    """
    ai í…Œì´ë¸”ì—ì„œ periodKeyì˜ ìµœì‹  1ê±´ì„ ê°€ì ¸ì˜¨ë‹¤.
    (PK=periodKey, SK=aiGeneratedAt)
    """
    table = DDB.Table(AI_TABLE)

    resp = table.query(
        KeyConditionExpression=Key("periodKey").eq(period_key),
        ScanIndexForward=False,
        Limit=1,
    )

    items = resp.get("Items", [])
    if not items:
        return {
            "periodKey": period_key,
            "found": False,
            "message": "NO_AI_RESULT",
        }

    item = items[0]

    # âœ… (1) ì°¨íŠ¸ ë°ì´í„°: INSIGHTS_TABLEì—ì„œ periodKey ê¸°ì¤€ ì „ì²´ shortId í•©ì‚°
    chart_period_key = "P#24H"
    time_bins = build_global_hourly_timebins(chart_period_key)
    # âœ… (2) ì¶”ì²œ ë°ì´í„°: AIê°€ ì¤€ top3ë§Œ ì‚¬ìš© (clicks ë¶™ì´ì§€ ì•ŠìŒ)
    raw_ai_insight = item.get("aiInsight") or {}
    top3 = []
    if isinstance(raw_ai_insight, dict) and isinstance(raw_ai_insight.get("top3"), list):
        # top3ê°€ ["15:20", ...] ì´ê±°ë‚˜ [{"time":"15:20"}, ...] ë‘˜ ë‹¤ ë°©ì–´
        for x in raw_ai_insight["top3"]:
            if isinstance(x, str):
                t = x.strip()
            elif isinstance(x, dict):
                t = str(x.get("time") or "").strip()
            else:
                t = ""
            if t and t not in top3:
                top3.append(t)
            if len(top3) >= 3:
                break

    # í”„ë¡ íŠ¸ê°€ ì›í•˜ëŠ” êµ¬ì¡°ë¡œ aiInsightë¥¼ "chart + recommendation" í˜•íƒœë¡œë§Œ ë‚´ë ¤ì¤Œ
    shaped_ai_insight = {
        "chart": {
            "timeBins": time_bins
        },
        "recommendation": {
            "top3": top3
        }
    }

    return {
        "found": True,
        "periodKey": item.get("periodKey"),
        "aiGeneratedAt": item.get("aiGeneratedAt"),

        # âœ… aiTrendëŠ” ì ˆëŒ€ ì•ˆ ê±´ë“œë¦¼ (ê·¸ëŒ€ë¡œ)
        "aiTrend": item.get("aiTrend"),

        # âœ… aiInsightë§Œ ì›í•˜ëŠ” í˜•íƒœë¡œ ë³€í™˜
        "aiInsight": shaped_ai_insight,
    }


def _get_query(event: dict, key: str, default=None):
    q = (event or {}).get("queryStringParameters") or {}
    v = q.get(key)
    return v if v not in (None, "") else default

def run_ai_job(ai_period_key: str, source_period_key: str):
    """
    AI ì „ìš© Job
    - source_period_key ê¸°ê°„(ê¸°ë³¸ P#24H) ë™ì•ˆì˜ í´ë¦­ ë¡œê·¸ë¥¼ ë°”íƒ•ìœ¼ë¡œ
      1) Trend (Nova Micro): ë„ë©”ì¸ TOP5 + ì¹´í…Œê³ ë¦¬ TOP5
      2) Insight (Nova Lite): ìµœì  ê³µìœ  ì‹œê°„ HH:MM TOP3
    - ê²°ê³¼ëŠ” ai í…Œì´ë¸”ì— ëˆ„ì  ì €ì¥ (PK=ai_period_key, SK=aiGeneratedAt)
    """

    # 1) ê¸°ê°„ ê³„ì‚°
    duration = parse_period_key(source_period_key)
    end_dt = now_utc()
    start_dt = end_dt - duration
    start_iso = iso(start_dt)
    end_iso = iso(end_dt)

    # 2) URL ëª©ë¡ ë¡œë“œ (ì´ˆê¸° ë‹¨ê³„ë‹ˆê¹Œ scan ê¸°ë°˜)
    urls = list_urls(MAX_URLS_PER_RUN)

    # 3) ì „ì—­ ì§‘ê³„
    top_url_clicks = Counter()  # normalizedUrl -> clicks
    domain_clicks = Counter()   # (ì„ íƒ) AI ê²°ê³¼ ê²€ì¦/ë°±ì—…ìš©ìœ¼ë¡œ ë‚¨ê²¨ë„ ë¨. í•„ìš”ì—†ìœ¼ë©´ ì‚­ì œ ê°€ëŠ¥

    time_bins = Counter()       # "HH:MM" -> clicks
    total_clicks_all = 0



    urls_sorted = sorted(urls, key=lambda x: safe_int(x.get("clickCount", 0)), reverse=True)

    # ë¹„ìš© ë°©ì§€: shortIdë‹¹ í´ë¦­ ë¡œê·¸ ìƒí•œ
    per_sid_limit = MAX_CLICKS_PER_SID if MAX_CLICKS_PER_SID > 0 else 0

    for u in urls_sorted:
        sid = u.get("shortId")
        if not sid:
            continue

        click_items = fetch_clicks_for_shortid(sid, start_iso, end_iso, limit=per_sid_limit)
        if not click_items:
            continue

        total_clicks_all += len(click_items)

        # ë„ë©”ì¸ ì§‘ê³„ (Trend ì…ë ¥ìš©)
        ou = normalize_url(u.get("originalUrl", ""))
        if ou:
            top_url_clicks[ou] += len(click_items)

        # 5ë¶„ ìŠ¬ë¡¯ ì§‘ê³„ (Insight ì…ë ¥ìš©)
        for it in click_items:
            ts = it.get("timestamp")
            if not ts:
                continue
            try:
                slot = to_5min_slot(ts)  # "HH:MM" (KST)
                time_bins[slot] += 1
            except Exception:
                pass

    # ë°ì´í„° ì—†ìœ¼ë©´ AI í˜¸ì¶œ ìŠ¤í‚µ
    if total_clicks_all == 0 or not top_url_clicks or not time_bins:
        return {
            "aiPeriodKey": ai_period_key,
            "sourcePeriodKey": source_period_key,
            "startAt": start_iso,
            "endAt": end_iso,
            "totalClicksSourceWindow": total_clicks_all,
            "skipped": True,
            "reason": "NO_DATA",
        }

    # 4) AI ì…ë ¥ Top-N ìƒì„±
    # - ë„ë©”ì¸ì€ AIê°€ top5 ë½‘ê²Œ í• ê±°ì§€ë§Œ, ì…ë ¥ì€ 20ê°œ ì •ë„ë©´ ì¶©ë¶„
    top_urls_input = top_url_clicks.most_common(AI_TOP_URL_N)
    top_bins_input = time_bins.most_common(AI_TOP_TIMEBIN_N)     # env ê¸°ë³¸ 10

    time_click_map = {t: int(c) for t, c in time_bins.items()}

    # 5) Trend í”„ë¡¬í”„íŠ¸ (ë„ë©”ì¸TOP5 + ì¹´í…Œê³ ë¦¬TOP5 ë¶„ë¦¬)
    trend_payload = {
        "topUrls": [{"url": url, "clicks": clicks} for url, clicks in top_urls_input]
    }

    trend_prompt = (
    "You are a classifier. Return ONLY valid JSON. No extra text.\n"
    "Task:\n"
    "1) From each input url, extract the domain in the form like 'youtube.com' or 'notion.so'.\n"
    "2) Aggregate clicks by domain.\n"
    "3) Choose top 5 domains by total clicks.\n"
    "4) Classify EACH top domain into ONE category from this fixed set:\n"
    "[video, blog, news, shopping, social, community, docs, dev, music, other]\n"
    "5) Produce topCategories by summing clicks of domains in the same category.\n"
    "Input JSON:\n"
    f"{json.dumps(trend_payload, ensure_ascii=False)}\n\n"
    "Output JSON schema:\n"
    "{"
    "\"topDomains\":[{\"domain\":\"example.com\",\"clicks\":123,\"category\":\"video\"}],"
    "\"topCategories\":[{\"category\":\"video\",\"clicks\":186}]"
    "}\n"
    "Rules:\n"
    "- topDomains length MUST be 5 (or less if distinct domains <5).\n"
    "- topCategories length MUST be 5 (or less if distinct categories <5).\n"
    "- Sort both lists by clicks desc.\n"
    "- Use only categories from the fixed set.\n"
)


    # 6) Insight í”„ë¡¬í”„íŠ¸ (HH:MM TOP3)
    insight_payload = {
        "topTimeBins": [{"time": t, "clicks": c} for t, c in top_bins_input]
    }

    insight_prompt = (
    "You are a recommender. Return ONLY valid JSON. No extra text.\n"
    "Timezone: Asia/Seoul (KST).\n"
    "Goal: output exactly 3 best share times in HH:MM.\n"
    "Input JSON:\n"
    f"{json.dumps(insight_payload, ensure_ascii=False)}\n\n"
    "Output JSON schema (MUST follow exactly):\n"
    "{\"top3\":[{\"time\":\"HH:MM\"},{\"time\":\"HH:MM\"},{\"time\":\"HH:MM\"}]}\n"
    "Rules (MUST follow):\n"
    "- top3 MUST contain exactly 3 items.\n"
    "- All 3 times MUST be unique.\n"
    "- Prefer times from the input list.\n"
    "- If input has fewer than 3 unique times, you MUST still output 3 unique times by generating\n"
    "  additional times in 5-minute steps around the best time.\n"
)


    # 7) Bedrock í˜¸ì¶œ
    ai_output = {}
    ai_trend_obj = None
    ai_insight_obj = None

    try:
        ai_output["trend_raw"] = bedrock_invoke_text(MODEL_TREND, trend_prompt, max_tokens=260)
        ai_output["insight_raw"] = bedrock_invoke_text(MODEL_INSIGHT, insight_prompt, max_tokens=120)

        # JSON íŒŒì‹± (í”„ë¡ íŠ¸ ê¹¨ì§ ë°©ì§€)
        ai_trend_obj = safe_json_obj(ai_output["trend_raw"])
        ai_insight_obj = safe_json_obj(ai_output["insight_raw"])

        # âœ… insight post-process:
        # - top3 unique ë³´ì¥
        # - DB ì§‘ê³„ clicks(time_bins/top_bins_input) ë§¤ì¹­í•´ì„œ ë¶™ì´ê¸°
        if isinstance(ai_insight_obj, dict) and isinstance(ai_insight_obj.get("top3"), list):
            uniq_times = []
            enriched = []

            for item in ai_insight_obj["top3"]:
                # itemì´ {"time":"HH:MM"} ë˜ëŠ” "HH:MM" ë‘˜ ë‹¤ ë°©ì–´
                if isinstance(item, dict):
                    t = str(item.get("time") or "").strip()
                else:
                    t = str(item or "").strip()

                if not t or t in uniq_times:
                    continue

                uniq_times.append(t)

                # ğŸ”¥ clicksëŠ” AIê°€ ì•„ë‹ˆë¼ DB ì§‘ê³„ê°’ì—ì„œ ë¶™ì„
                clicks = int(time_click_map.get(t, 0))

                enriched.append({
                    "time": t,
                    "clicks": clicks,
                })

                if len(enriched) >= 3:
                    break

            ai_insight_obj["top3"] = enriched


        # íŒŒì‹± ì‹¤íŒ¨(raw)ë©´ ì €ì¥í•˜ì§€ ì•Šê¸°
        if isinstance(ai_trend_obj, dict) and "raw" in ai_trend_obj:
            ai_trend_obj = None
        if isinstance(ai_insight_obj, dict) and "raw" in ai_insight_obj:
            ai_insight_obj = None

        # ai í…Œì´ë¸” ëˆ„ì  ì €ì¥ (PK=ai_period_key)
        put_ai_result(ai_period_key, ai_trend_obj, ai_insight_obj)

    except Exception as e:
        print(json.dumps({"type": "AI_JOB_ERROR", "error": str(e)}, ensure_ascii=False))


    return {
        "aiPeriodKey": ai_period_key,
        "sourcePeriodKey": source_period_key,
        "startAt": start_iso,
        "endAt": end_iso,
        "totalClicksSourceWindow": total_clicks_all,
        "input": {
            "urlsTopN": top_urls_input[:5],
            "timeBinsTopN": top_bins_input[:5],
        },
        "output": {
            "trend": ai_trend_obj,
            "insight": ai_insight_obj,
        },
        "raw": {
            "trend": ai_output.get("trend_raw"),
            "insight": ai_output.get("insight_raw"),
        }
    }

def safe_int(x):
    try:
        return int(x)
    except Exception:
        return 0

def run_aggregation(period_key: str):
    duration = parse_period_key(period_key)
    end_dt = now_utc()
    start_dt = end_dt - duration
    start_iso = iso(start_dt)
    end_iso = iso(end_dt)

    urls = list_urls(MAX_URLS_PER_RUN)

    

    # âœ… 0) S3 Export (ì‹ ê·œ í´ë¦­ë§Œ) - ìŠ¤í‚¤ë§ˆ ë³€ê²½ ì—†ì´ fact_clicks ìƒì„±
    if EXPORT_ENABLED and ANALYTICS_BUCKET and period_key == "P#1H":
        try:
            last_ts = load_export_checkpoint()
            export_start = last_ts or start_iso  # âœ… ì²« ì‹¤í–‰ì€ í˜„ì¬ ì§‘ê³„ windowë§Œ export
            run_id = f"{iso(end_dt).replace(':','').replace('-','')}-{uuid.uuid4().hex[:8]}"

            export_records = []

            # urlsëŠ” ì´ë¯¸ scan ê²°ê³¼ì´ë¯€ë¡œ ì—¬ê¸°ì„œ shortIdë§Œ ëŒë©´ ë¨
            for u in urls:
                sid = u.get("shortId")
                if not sid:
                    continue

                # ì‹ ê·œ í´ë¦­ë§Œ
                items = fetch_clicks_for_shortid(sid, export_start, end_iso, limit=MAX_CLICKS_PER_SID)
                if not items:
                    continue

                for it in items:
                    export_records.append(click_to_fact_record(sid, it))

            # íŒŒì¼ ì €ì¥(ë°ì´í„° ì—†ìœ¼ë©´ íŒŒì¼ì€ ìƒëµ)
            if export_records:
                export_fact_clicks_jsonl(export_records, end_dt=end_dt, run_id=run_id)

            # âœ… ë°ì´í„° ì—†ë”ë¼ë„ ì²´í¬í¬ì¸íŠ¸ëŠ” ì•ìœ¼ë¡œ ì´ë™(ì¬ì¡°íšŒ ë°©ì§€)
            save_export_checkpoint(end_iso)

        except Exception as e:
            # export ì‹¤íŒ¨ê°€ ì§‘ê³„/insights ì—…ë°ì´íŠ¸ë¥¼ ë§‰ì§€ ì•Šê²Œ
            print(json.dumps({"type": "S3_EXPORT_ERROR", "error": str(e)}, ensure_ascii=False))
    

    urls_sorted = sorted(urls, key=lambda x: safe_int(x.get("clickCount", 0)), reverse=True)

    processed = 0
    total_clicks_all = 0

    notified = 0
    alert_state = {}
    if period_key == ALERT_ONLY_PERIOD:
        try:
            alert_state = load_alert_state()
            print(json.dumps({
                "type": "ALERT_STATE_LOADED",
                "periodKey": period_key,
                "count": len(alert_state)
            }, ensure_ascii=False))
        except Exception as e:
            print(json.dumps({
                "type": "ALERT_STATE_LOAD_ERROR",
                "error": str(e)
            }, ensure_ascii=False))
            alert_state = {}
    

    for u in urls_sorted:
        sid = u.get("shortId")
        if not sid:
            continue

        click_items = fetch_clicks_for_shortid(sid, start_iso, end_iso)
        total, by_hour, by_day, by_ref, by_device = aggregate(click_items)
        suspicious_clicks = compute_suspicious(click_items)

        print(json.dumps({
            "type": "SUSP_CHECK",
            "sid": sid,
            "total": total,
            "suspicious_clicks": suspicious_clicks,
            "periodKey": period_key,
            }, ensure_ascii=False))

        # âœ… Slack alert: P#1Hì—ì„œë§Œ + suspicious ì¦ê°€í–ˆì„ ë•Œë§Œ
        if period_key == ALERT_ONLY_PERIOD and suspicious_clicks > 0:
            last_alerted = int(alert_state.get(sid, 0))

            print(json.dumps({
                "type": "SLACK_ALERT_CHECK",
                "sid": sid,
                "periodKey": period_key,
                "suspicious_clicks": suspicious_clicks,
                "last_alerted": last_alerted,
                "notified": notified,
                "limit": MAX_ALERTS_PER_RUN
            }, ensure_ascii=False))

            if suspicious_clicks > last_alerted:
                if notified < MAX_ALERTS_PER_RUN:
                    suspect_rate = (suspicious_clicks / total) if total else 0.0

                    print(json.dumps({
                        "type": "SLACK_ALERT_TRY",
                        "sid": sid,
                        "periodKey": period_key,
                        "suspicious_clicks": suspicious_clicks,
                        "last_alerted": last_alerted,
                        "total": total,
                        "notified_before": notified,
                    }, ensure_ascii=False))

                    start_kst = iso_to_kst_display(start_iso)
                    end_kst = iso_to_kst_display(end_iso)

                    text = (
                        f"âš ï¸ ë¹„ì •ìƒ í´ë¦­ ê°ì§€ (ì¦ê°€)\n"
                        f"- shortId: `{sid}`\n"
                        f"- periodKey: {period_key}\n"
                        f"- suspiciousClicks: {suspicious_clicks} (prev: {last_alerted})\n"
                        f"- totalClicks: {total}\n"
                        f"- suspiciousRate: {suspect_rate:.0%}\n"
                        f"- window(KST): {start_kst} ~ {end_kst}"
                    )

                    ok = send_slack(text)

                    if ok:
                        # âœ… ì„±ê³µí–ˆì„ ë•Œë§Œ ë§ˆì§€ë§‰ ì•Œë¦¼ê°’ ê°±ì‹ 
                        alert_state[sid] = suspicious_clicks

                        # âœ… ì¦‰ì‹œ ì €ì¥ (ì¤‘ê°„ ì‹¤íŒ¨/íƒ€ì„ì•„ì›ƒ ëŒ€ë¹„)
                        try:
                            save_alert_state(alert_state)
                            print(json.dumps({
                                "type": "ALERT_STATE_SAVED_IMMEDIATE",
                                "sid": sid,
                                "periodKey": period_key,
                                "new_last_alerted": suspicious_clicks,
                                "key": ALERT_STATE_KEY
                            }, ensure_ascii=False))
                        except Exception as e:
                            print(json.dumps({
                                "type": "ALERT_STATE_SAVE_IMMEDIATE_ERROR",
                                "sid": sid,
                                "periodKey": period_key,
                                "error": str(e),
                                "key": ALERT_STATE_KEY
                            }, ensure_ascii=False))

                        print(json.dumps({
                            "type": "SLACK_ALERT_SENT",
                            "sid": sid,
                            "periodKey": period_key,
                            "new_last_alerted": suspicious_clicks,
                            "notified_before": notified,
                        }, ensure_ascii=False))
                        notified += 1
                    else:
                        print(json.dumps({
                            "type": "SLACK_ALERT_NOT_SENT",
                            "sid": sid,
                            "periodKey": period_key,
                            "notified": notified
                        }, ensure_ascii=False))
                else:
                    print(json.dumps({
                        "type": "SLACK_SKIPPED_BY_LIMIT",
                        "sid": sid,
                        "limit": MAX_ALERTS_PER_RUN,
                        "notified": notified
                    }, ensure_ascii=False))
            else:
                # âœ… ì¦ê°€ ì•ˆ í–ˆìœ¼ë©´ ìŠ¤í‚µ (ì¤‘ë³µ ë°©ì§€ í•µì‹¬)
                print(json.dumps({
                    "type": "SLACK_SKIPPED_NOT_INCREASED",
                    "sid": sid,
                    "periodKey": period_key,
                    "suspicious_clicks": suspicious_clicks,
                    "last_alerted": last_alerted
                }, ensure_ascii=False))

        print("DEBUG_AGG_RESULT", sid, period_key, total, by_hour, by_day, by_ref, by_device)

        upsert_insight(
            short_id=sid,
            period_key=period_key,
            start_at=start_iso,
            end_at=end_iso,
            total=total,
            by_hour=by_hour,
            by_day=by_day,
            by_ref=by_ref,
            by_device=by_device,
            suspicious_clicks=suspicious_clicks,
        )

        total_clicks_all += total
        processed += 1
    
        # âœ… P#1H ì•Œë¦¼ ìƒíƒœ ì €ì¥
    if period_key == ALERT_ONLY_PERIOD:
        try:
            save_alert_state(alert_state)
            print(json.dumps({
                "type": "ALERT_STATE_SAVED",
                "periodKey": period_key,
                "count": len(alert_state)
            }, ensure_ascii=False))
        except Exception as e:
            print(json.dumps({
                "type": "ALERT_STATE_SAVE_ERROR",
                "error": str(e)
            }, ensure_ascii=False))

    # ì»¤ìŠ¤í…€ ë©”íŠ¸ë¦­(ê°œë°œì ëª¨ë‹ˆí„°ë§ìš©)
    put_custom_metrics(
        namespace="UrlShortener/Analytics",
        metrics={
            "ProcessedUrls": processed,
            "TotalClicksWindow": total_clicks_all,
        },
        dims=[{"Name": "PeriodKey", "Value": period_key}],
    )

    return {
        "periodKey": period_key,
        "startAt": start_iso,
        "endAt": end_iso,
        "processedUrls": processed,
        "totalClicksWindow": total_clicks_all,
    }

def _json_default(o):
    if isinstance(o, Decimal):
        # ì •ìˆ˜ë©´ intë¡œ, ì†Œìˆ˜ë©´ floatë¡œ
        if o % 1 == 0:
            return int(o)
        return float(o)
    raise TypeError(f"Type not serializable: {type(o)}")

def _resp(status: int, body_obj: dict):
    return {
        "statusCode": status,
        "headers": {
            "Content-Type": "application/json",
            "Access-Control-Allow-Origin": "*",
            "Access-Control-Allow-Methods": "GET,POST,OPTIONS",
            "Access-Control-Allow-Headers": "Content-Type,Authorization",
        },
        "body": json.dumps(body_obj, ensure_ascii=False, default=_json_default),
    }

def _s3_get_json(bucket: str, key: str):
    try:
        resp = S3.get_object(Bucket=bucket, Key=key)
        body = resp["Body"].read().decode("utf-8")
        return json.loads(body) if body else None
    except ClientError as e:
        code = e.response.get("Error", {}).get("Code")
        if code in ("NoSuchKey", "404"):
            return None
        print(json.dumps({"type": "S3_GET_JSON_ERROR", "bucket": bucket, "key": key, "error": str(e)}, ensure_ascii=False))
        return None


def _s3_put_json(bucket: str, key: str, obj: dict):
    S3.put_object(
        Bucket=bucket,
        Key=key,
        Body=(json.dumps(obj, ensure_ascii=False) + "\n").encode("utf-8"),
        ContentType="application/json",
    )


def load_export_checkpoint() -> str | None:
    if not (ANALYTICS_BUCKET and EXPORT_CHECKPOINT_KEY):
        return None
    obj = _s3_get_json(ANALYTICS_BUCKET, EXPORT_CHECKPOINT_KEY)
    if isinstance(obj, dict):
        ts = obj.get("lastExportTs")
        if isinstance(ts, str) and ts:
            return ts
    return None


def save_export_checkpoint(ts_iso: str):
    if not (ANALYTICS_BUCKET and EXPORT_CHECKPOINT_KEY):
        return
    _s3_put_json(ANALYTICS_BUCKET, EXPORT_CHECKPOINT_KEY, {"lastExportTs": ts_iso})


def click_to_fact_record(short_id: str, it: dict) -> dict:
    ts = it.get("timestamp") or ""
    referer = it.get("referer") or "direct"
    ua = it.get("userAgent") or ""
    ip_hash = it.get("ip") or ""

    device = classify_device(ua)
    # âœ… ìµœì†Œ ë²„ì „: bot UAë©´ suspect ì²˜ë¦¬ (burst ë£°ê¹Œì§€ ì´ë²¤íŠ¸ ë‹¨ìœ„ë¡œ ì°ëŠ” ê±´ ë‚˜ì¤‘ì— í™•ì¥ ê°€ëŠ¥)
    is_suspect = bool(ua and BOT_UA_PAT.search(ua))

    return {
        "ts": ts,                 # ISO string (Z)
        "shortId": short_id,
        "referer": referer,
        "device": device,
        "isSuspect": is_suspect,
        # ì„ íƒ ì»¬ëŸ¼(ìˆìœ¼ë©´ 6C ë””ë²„ê¹…/í•„í„°ì— ë„ì›€)
        "ipHash": ip_hash,
        "userAgent": ua,
    }


def export_fact_clicks_jsonl(records: list[dict], end_dt: datetime, run_id: str):
    """
    S3ì— JSON Linesë¡œ ì €ì¥:
    s3://{bucket}/{prefix}/fact_clicks/dt=YYYY-MM-DD/hr=HH/run_id.jsonl
    """
    if not (ANALYTICS_BUCKET and ANALYTICS_PREFIX):
        return

    dt = end_dt.strftime("%Y-%m-%d")
    hr = end_dt.strftime("%H")

    key = f"{ANALYTICS_PREFIX}/fact_clicks/dt={dt}/hr={hr}/{run_id}.jsonl"
    body = "\n".join(json.dumps(r, ensure_ascii=False) for r in records) + "\n"

    S3.put_object(
        Bucket=ANALYTICS_BUCKET,
        Key=key,
        Body=body.encode("utf-8"),
        ContentType="application/x-ndjson",
    )

    print(json.dumps({
        "type": "S3_EXPORT_OK",
        "bucket": ANALYTICS_BUCKET,
        "key": key,
        "records": len(records),
    }, ensure_ascii=False))


def send_slack(text: str) -> bool:
    webhook_url = os.getenv("SLACK_WEBHOOK_URL", "").strip()

    print(json.dumps({
        "type": "SLACK_FUNC_ENTER",
        "has_webhook": bool(webhook_url),
        "text_preview": text[:80]
    }, ensure_ascii=False))

    if not webhook_url:
        print(json.dumps({"type": "SLACK_SKIP_NO_WEBHOOK"}, ensure_ascii=False))
        return False

    req = urllib.request.Request(
        webhook_url,
        data=json.dumps({"text": text}).encode("utf-8"),
        headers={"Content-Type": "application/json"},
        method="POST",
    )

    try:
        with urllib.request.urlopen(req, timeout=5) as resp:
            status = getattr(resp, "status", None) or resp.getcode()
            body = resp.read().decode("utf-8", errors="ignore")
            print(json.dumps({
                "type": "SLACK_HTTP_OK",
                "status": status,
                "body_preview": body[:200]
            }, ensure_ascii=False))
            return True

    except urllib.error.HTTPError as e:
        err_body = e.read().decode("utf-8", errors="ignore")
        print(json.dumps({
            "type": "SLACK_HTTP_ERROR",
            "status": e.code,
            "reason": str(e.reason),
            "body": err_body[:500]
        }, ensure_ascii=False))
        return False

    except Exception as e:
        print(json.dumps({
            "type": "SLACK_SEND_EXCEPTION",
            "error": str(e)
        }, ensure_ascii=False))
        return False

def iso_to_kst_display(ts_iso: str) -> str:
    """
    '2026-02-23T14:22:48Z' -> '2026-02-23 23:22:48 KST'
    """
    if not ts_iso:
        return "-"
    try:
        dt_utc = datetime.strptime(ts_iso, "%Y-%m-%dT%H:%M:%SZ").replace(tzinfo=timezone.utc)
        dt_kst = dt_utc.astimezone(KST)
        return dt_kst.strftime("%Y-%m-%d %H:%M:%S KST")
    except Exception:
        return ts_iso
    

def load_alert_state() -> dict:
    """
    shortIdë³„ ë§ˆì§€ë§‰ ì•Œë¦¼ suspiciousClicks ì €ì¥ê°’ ë¡œë“œ
    ì˜ˆ: {"abc123": 2, "def456": 5}
    """
    if not (ANALYTICS_BUCKET and ALERT_STATE_KEY):
        return {}

    obj = _s3_get_json(ANALYTICS_BUCKET, ALERT_STATE_KEY)
    if isinstance(obj, dict):
        # ê°’ ì •ë¦¬ (ìˆ«ì ì•„ë‹Œ ê°’ ë°©ì–´)
        cleaned = {}
        for k, v in obj.items():
            try:
                cleaned[str(k)] = int(v)
            except Exception:
                continue
        return cleaned
    return {}


def save_alert_state(state: dict):
    if not (ANALYTICS_BUCKET and ALERT_STATE_KEY):
        return
    _s3_put_json(ANALYTICS_BUCKET, ALERT_STATE_KEY, state)